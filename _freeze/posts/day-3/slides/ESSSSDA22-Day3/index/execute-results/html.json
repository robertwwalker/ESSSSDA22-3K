{
  "hash": "fef7e046c2e9b8aec71311db985e34c4",
  "result": {
    "markdown": "---\ntitle: \"Day 3: Heterogeneity and Dynamics in Data\"\nsubtitle: \"Dynamic Regression Models\"  \nauthor: \"Robert W. Walker\"\ndate: \"August 8, 2022\"\noutput:\n  xaringan::moon_reader:\n    css: xaringan-themer.css\n    nature:\n      slideNumberFormat: \"%current%\"\n      highlightStyle: github\n      highlightLines: true\n      ratio: 16:9\n      countIncrementalSlides: true\n      beforeInit: \"https://platform.twitter.com/widgets.js\"\n\n---\n\n\n\n\n\n\n\n\n# Outline for Day 3\n\n1. Dynamic Models  \n2. GLS vs. OLS and fixes\n\n---\n\n## Day 3: Dynamic Models\n\nThe ARIMA approach is fundamentally inductive.  The workflow involves the use of empirical values of ACFs and PACFs to engage in model selection.  Dynamic models engage theory/structure to impose more stringent assumptions for producing estimates.\n\n---\n\n## Time Series Linear Models/Dynamic Models\n\nFirst, a result.  **Aitken Theorem**\n\nIn a now-classic paper, Aitken generalized the Gauss-Markov theorem to the class of Generalized Least Squares estimators.  It is important to note that these are GLS and not FGLS estimators.  What is the difference?  The two GLS estimators considered by Stimson are not strictly speaking GLS.\n\nDefinition $$\\hat{\\beta}_{GLS} = (\\mathbf{X}^{\\prime}\\Omega^{-1}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}\\Omega^{-1}\\mathbf{y}$$\n> Properties \n>  \n> (1) GLS is unbiased.  \n> (2) Consistent.  \n> (3) Asymptotically normal.  \n> (4) MV(L)UE\n\n---\n\n## A Quick Example\n\nThe variance/covariance matrix of the errors for a first-order autoregressive process is useful to derive.  \n\n1. The matrix is banded; observations separated by one point in time are correlated $\\rho$.  Period two is $\\rho^2$; the corners are $\\rho^{T-1}$.  The diagonal is one.  \n\n--\n\n1. What I have actually described is the correlation; the relevant autocovariances are actually defined by $\\frac{\\sigma^{2}\\rho^{s}}{1 - \\rho^2}$ where $s$ denotes the time period separation.  \n\n--\n\n1. It is also straightforward to prove (tediously through induction) that this is invertible; it is square and the determinant is non-zero having assumed that $|\\rho < 1|$.  \n   -  The $2x2$ determinant is $\\frac{1}{1-\\rho^2}$.  \n   -  The $3x3$ is $1*(1-\\rho^2) - \\rho(\\rho - \\rho^3) + \\rho^2(\\rho^2 - \\rho^2)$.  The first term is positive and the second term is non-zero so long as $\\rho \\neq 0$.  But even if $\\rho=0$, we would have an identity matrix which is invertible.\n\n---\n\n\n$$\\Phi = \\sigma^{2}\\Psi = \\sigma^{2}_{e}  \\left(\\begin{array}{ccccc}1 & \\rho^{1} & \\rho^{2} & \\ldots & \\rho^{T-1} \\\\ \\rho^1 & 1 & \\rho^1 & \\ldots & \\rho^{T-2} \\\\ \\rho^{2} & \\rho^1 & 1 & \\ldots & \\rho^{T-3} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho^{T-1} & \\rho^{T-2} & \\rho^{T-3} & \\ldots & 1 \\end{array}\\right)$$\n\n\ngiven that $e_{t} = \\rho e_{t-1} + \\nu_{t}$.  A Toeplitz form....\n\nIf the variance is stationary, we can rewrite,\n\n$$\\sigma^{2}_{e} = \\frac{\\sigma^{2}_{\\nu}}{1 - \\rho^{2}}$$\n\n\nA comment on characteristic roots....\n\n---\n\n## Cochrane-Orcutt\n\nWe have the two key elements to implement this except that we do not know $\\rho$; we will have to estimate it and estimates have uncertainty.  But it is important to note this imposes exactly an AR(1).  If the process is incorrectly specified, then the optimal properties do not follow.  Indeed, the optimal properties also depend on an additional important feature.\n\n---\n\n## What does the feasible do?\n\nWe need to estimate things to replace unknown covariance structures\nand coverage will depend on properties of the estimators of these\ncovariances.  \n\n- Consistent estimators will work but there is\neuphemistically `considerable variation` in the class of consistent\nestimators.  \n\n- Contrasting the Beck and Katz/White approach with the GLS\napproach is a valid difference in philosophies.<sup>1</sup>  One takes advantage of OLS and Basus Theorem; one goes full Aitken.\n\n.footnote[<sup>1</sup> We will return to this when we look at Hausman because this is the essential issue.]\n\n---\n\n## Incremental Models\n\n\n$$y_{t} = a_{1} y_{t-1} + \\epsilon_{t}$$\n\n\nis the simplest dynamic model but it cannot be estimated consistently, in general terms, in the presence of serial correlation.  .red[Why?]\n\n--\nThe key condition for unbiasedness is violated because $\\mathbb{E}(y_{t-1}\\epsilon_{t}) \\neq 0$.  OLS will not generally work.\n\n**A note on dynamic interpretation.**\n\n---\n\n## Incremental Models with Covariates\n\n\n$$y_{t} = a_{1} y_{t-1} + \\beta X_t + \\epsilon_{t}$$\n\n\nThe problem is fitting and the key issue is white noise residuals post-estimation.  But we have to assume a structure and implement it.\n\n---\n\n## Distributed Lag Models\n\n\n$$y_{t} = \\alpha + \\beta_{0} X_t + \\beta_{1}x_{t-1} + \\ldots + \\epsilon_{t}$$\n\n\nThe impact of $x$ occurs over multiple periods.  It relies on theory, or perhaps analysis using information criteria/F [owing to quasi-nesting and missing data].  OLS is a fine solution to this problem but the search space of models is often large.\n\nIn response to this problem, we have structured distributed lag models; there are many such schemes.\n\n- Koyck/Geometric decay:  \n\n--\nshort run and long-run effects are parametrically identified\n\n$$y_t = \\alpha + \\beta(1-\\lambda)\\sum_{j=0}^{\\infty}\\lambda^{j}X_{t-j} + \\epsilon$$\n\n\n--\n- Almon (more arbitrary decay)\n\n$$y_{it} = \\sum_{t_{A}=0}^{T_{F}} \\rho_{t_{A}}x_{t - t_{A}} + \\epsilon_{t}$$ with coefficients that are ordinates of some general polynomial of degree $T_{F} >> q$.  The $\\rho_{t_{A}} = \\sum_{k=0}^{T_{F}} \\gamma_{k}t^{k}$.\n\n\n---\n\n## Autoregressive Distributed Lag Models\n\n\n$$y_{t} = \\alpha + \\gamma_{1}y_{t-1} + \\beta_{0} X_t + \\beta_{1}X_{t-1} + \\beta_{2}X_{t-2} + \\ldots + \\epsilon_{t}$$\n\n\n- OLS is often used if iid; $\\epsilon_{t}$ is unrelated to $y_{t-1}$ is common if nonsensical.\n- If not iid: GLS is needed.\n- The authors argue that the lagged dependent variable often yields white noise for free.  As they also note, there is a deBoef and Keele paper showing the relationship between these models and a form of error correction models.  More on that tomorrow.\n- There is substance to the timing of impacts.\n\n---\n\n## Structural vs. Non-structural\n\nData analysis can quite yield models comparisons among competing dynamic structures.  The key issue is that the analyst need divine the process; what is the relevant error process and what is the structure and timing of effects alongside the potential question of incremental adjustment.  We need good theory for that.\n\nGiven such theory, we can take an equations as analysis approach, measure the variables, and derive reduced forms, and then recover parameter estimates deploying simultaneous equations methods.  Very large such systems were a core part of early empirical macroeconomics.  The failures of such systems led to the proposal of alternatives.\n\nChris Sims suggested a more flexible approach: the VAR.  \n\n---\n\n## VAR: Vector AutoRegression\n\n- Choose a relevant set of lag lengths and write each variable in the system as a function of lags of itself and other variables to the chosen lengths.<sup>1</sup> [For Stata](https://www.stata.com/manuals/tsvar.pdf) and [for R](https://otexts.com/fpp3/VAR.html)\n\n.footnote[<sup>1</sup> A nice blog post with an extended example in R can be found on [towardsdatascience](https://towardsdatascience.com/a-deep-dive-on-vector-autoregression-in-r-58767ebb3f06).  Kit Baum has [a similar worked example in slides](http://fmwww.bc.edu/EC-C/S2016/8823/ECON8823.S2016.nn10.slides.pdf).]\n\n--\n\n- The key insight is that this VAR is the reduced form to some more complicated as yet unspecified structural form.  \n\n--\n\n- But if the goal is to specify how variables related to one another and to use data to discover Granger causality and responses to impulse injected in the system.\n\n---\n\n## A very simple example\n\n```\nlibrary(forecast)\nmdeaths\nfdeaths\nsave(mdeaths, fdeaths, file = \"./img/LungDeaths.RData\")\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(hrbrthemes)\nload(url(\"https://github.com/robertwwalker/Essex-Data/raw/main/LungDeaths.RData\"))\nMales <- mdeaths; Females <- fdeaths\nLung.Deaths <- cbind(Males, Females) %>% as_tsibble()\nLung.Deaths %>% autoplot() + theme_ipsum_rc()\n```\n:::\n\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.svg){width=100%}\n:::\n:::\n\n---\n\n.left-column[\n::: {.cell}\n\n```{.r .cell-code}\nlung_deaths <- cbind(mdeaths, fdeaths) %>%\n  as_tsibble(pivot_longer = FALSE)\nfit <- lung_deaths %>%\n  model(VAR(vars(mdeaths, fdeaths) ~ AR(3)))\nreport(fit)\n```\n:::\n]\n.right-column[\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nSeries: mdeaths, fdeaths \nModel: VAR(3) w/ mean \n\nCoefficients for mdeaths:\n      lag(mdeaths,1)  lag(fdeaths,1)  lag(mdeaths,2)  lag(fdeaths,2)\n              0.6675          0.8074          0.3677         -1.4540\ns.e.          0.3550          0.8347          0.3525          0.8088\n      lag(mdeaths,3)  lag(fdeaths,3)  constant\n              0.2606         -1.1214  538.7817\ns.e.          0.3424          0.8143  137.1047\n\nCoefficients for fdeaths:\n      lag(mdeaths,1)  lag(fdeaths,1)  lag(mdeaths,2)  lag(fdeaths,2)\n              0.2138          0.4563          0.0937         -0.3984\ns.e.          0.1460          0.3434          0.1450          0.3328\n      lag(mdeaths,3)  lag(fdeaths,3)  constant\n              0.0250          -0.315  202.0027\ns.e.          0.1409           0.335   56.4065\n\nResidual covariance matrix:\n         mdeaths  fdeaths\nmdeaths 58985.95 22747.94\nfdeaths 22747.94  9983.95\n\nlog likelihood = -812.35\nAIC = 1660.69\tAICc = 1674.37\tBIC = 1700.9\n```\n:::\n:::\n]\n\n---\n\n.left-column[\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- lung_deaths %>%\n  model(VAR(vars(mdeaths, fdeaths) ~ AR(2)))\nreport(fit2)\n```\n:::\n]\n.right-column[\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nSeries: mdeaths, fdeaths \nModel: VAR(2) w/ mean \n\nCoefficients for mdeaths:\n      lag(mdeaths,1)  lag(fdeaths,1)  lag(mdeaths,2)  lag(fdeaths,2)  constant\n              0.9610          0.3340          0.1149         -1.3379  443.8492\ns.e.          0.3409          0.8252          0.3410          0.7922  124.4608\n\nCoefficients for fdeaths:\n      lag(mdeaths,1)  lag(fdeaths,1)  lag(mdeaths,2)  lag(fdeaths,2)  constant\n              0.3391          0.2617         -0.0601         -0.2691  145.0546\ns.e.          0.1450          0.3510          0.1450          0.3369   52.9324\n\nResidual covariance matrix:\n         mdeaths  fdeaths\nmdeaths 62599.51 24942.79\nfdeaths 24942.79 11322.70\n\nlog likelihood = -833.17\nAIC = 1694.35\tAICc = 1701.98\tBIC = 1725.83\n```\n:::\n:::\n]\n\n\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nfit %>%\n  fabletools::forecast(h=12) %>%\n  autoplot(lung_deaths)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.svg){width=100%}\n:::\n:::\n\n\n---\nclass: inverse\n\n### Female\n\n.left-column[\n::: {.cell}\n\n```{.r .cell-code}\nlung_deaths %>%\nmodel(VAR(vars(mdeaths, fdeaths) ~ AR(3))) %>%\n  residuals() %>% \n  pivot_longer(., cols = c(mdeaths,fdeaths)) %>% \n  filter(name==\"fdeaths\") %>% \n  as_tsibble(index=index) %>% \n  gg_tsdisplay(plot_type = \"partial\") + labs(title=\"Female residuals\n```\n:::\n]\n.right-column[\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.svg){width=100%}\n:::\n:::\n]\n\n---\nclass: inverse\n\n### Male\n\n.left-column[\n::: {.cell}\n\n```{.r .cell-code}\nlung_deaths %>%\nmodel(VAR(vars(mdeaths, fdeaths) ~ AR(3))) %>%\n  residuals() %>% \n  pivot_longer(., cols = c(mdeaths,fdeaths)) %>% \n  filter(name==\"mdeaths\") %>% \n  as_tsibble(index=index) %>% \n  gg_tsdisplay(plot_type = \"partial\") + labs(title=\"Male residuals\n```\n:::\n]\n.right-column[\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.svg){width=100%}\n:::\n:::\n]\n\n---\n\n## Easy Impulse Response\n\n**What happens if I shock one of the series; how does it work through the system?**  \n\nThe idea behind an impulse-response is core to counterfactual analysis with time series.  What does our future world look like and what predictions arise from it and the model we have deployed?\n\n--\n\nWhether VARs or dynamic linear models or ADL models, these are key to interpreting a model **in the real world**.\n\n---\n\n### Males\n\n.left-column[\n::: {.cell}\n\n```{.r .cell-code}\nVARMF <- cbind(Males,Females)\nmod1 <- vars::VAR(VARMF, p=3, type=\"const\")\nplot(vars::irf(mod1, boot=TRUE, impulse=\"Males\"))\n```\n:::\n]\n.right-column[\n![](./img/MIRF.png)\n]\n\n---\n\n### Females\n\n.left-column[\n::: {.cell}\n\n```{.r .cell-code}\nplot(vars::irf(mod1, boot=TRUE, impulse=\"Females\"))\n```\n:::\n]\n.right-column[\n![](./img/FIRF.png)\n]",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}